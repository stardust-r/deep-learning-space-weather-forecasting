{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp stats_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import boxcox, pearsonr\n",
    "from scipy.special import inv_boxcox\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tools.eval_measures import aic, bic\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class cPreProcessing():\n",
    "    \"\"\"\n",
    "    Parent class.\n",
    "    \n",
    "    Methods for dealing with irregularly spaced or missing data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fget_regular_times(self, df, timestep):\n",
    "        \"\"\"\n",
    "        Generate dataframe of regularly spaced times (to impute to)\n",
    "        (From fbprophet/forecaster/make_future_dataframe)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df       = [pandas dataframe]\n",
    "        timestep = [datetime timedelta object]\n",
    "        \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        regtimes = [pandas DatetimeIndex] of datetimes regularly spaced at timestep\n",
    "        \n",
    "        \"\"\"\n",
    "        # normalize start date to midnight\n",
    "        start_date = df.ds.min().normalize()\n",
    "\n",
    "        # round up end date by one extra timestep\n",
    "        end_date = (df.ds.max() + timestep).normalize()\n",
    "\n",
    "        # https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.date_range.html\n",
    "        regtimes = pd.date_range(start=start_date, end=end_date, freq=to_offset(timestep))\n",
    "\n",
    "        return regtimes\n",
    "    \n",
    "    def finterleaf(self, df, impute_times):\n",
    "        \"\"\"\n",
    "        Interleaf dataframe with new prediction times\n",
    "        Set values at prediction dates as NaN so can use imputer\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df           = [pandas dataframe]\n",
    "        impute_times = [pandas DatetimeIndex] (format of regtimes)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        dft = pandas dataframe (format for use in fimpute)\n",
    "        \"\"\"\n",
    "        # if impute_times taken from fmake_regular_times()\n",
    "        if type(impute_times) == pd.core.indexes.datetimes.DatetimeIndex:\n",
    "            impute_times = pd.DataFrame(impute_times)\n",
    "            impute_times.columns = [\"ds\"]\n",
    "\n",
    "        # set date index\n",
    "        df.set_index('ds', inplace=True)\n",
    "        impute_times.set_index('ds', inplace=True)\n",
    "\n",
    "        # combine (interleaf)\n",
    "        dft = pd.concat([df, impute_times], sort=True)\n",
    "        dft.sort_values(by=[\"ds\"], inplace=True)\n",
    "        \n",
    "        # remove duplicate entries\n",
    "        dft = dft[dft.index.duplicated() == False]\n",
    "\n",
    "        return dft\n",
    "    \n",
    "    def fimpute(self, df, method=\"time\"):\n",
    "        \"\"\"\n",
    "        Imputation of data to new timestamps with NaN value.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df = dataframe containing original data and NaNs at timestamps for imputation\n",
    "             timestamps are the df index\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dfi = imputed dataframe\n",
    "        \"\"\"\n",
    "        # interpolates halfway, doesn´t account for weighting towards closer time\n",
    "        if method == \"interp\":\n",
    "            dfi = df.interpolate()\n",
    "\n",
    "        # missing values given mean value over whole time series\n",
    "        if method == \"mean\":\n",
    "            imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            imp.fit(df)\n",
    "            dfi = imp.transform(df)\n",
    "\n",
    "        # linear interpolation weighted by timestamp proximity\n",
    "        if method == \"time\":\n",
    "            dfi = df.interpolate(method='time') \n",
    "\n",
    "        # smoothing\n",
    "        if method == \"akima\":\n",
    "            dfi = df.interpolate(method='akima')\n",
    "\n",
    "        return dfi\n",
    "    \n",
    "    def fmake_regular_freq(self, df, timestep, method=\"time\"):\n",
    "        \"\"\"\n",
    "        Interpolate data so regular update frequency throughout dataset.\n",
    "        (Deal with missing values)\n",
    "        \n",
    "        Some python functions (e.g. seasonal_decompose, AutoArima) require a data \"freq\" argument\n",
    "        to determine seasonality. (Can be inferred from df.index.freq, df.index.inferred_freq)\n",
    "        Such functions require a constant data frequency.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df       = irregularly space dataframe (with datestamp name \"ds\")\n",
    "        timestep = desired update frequency of data (timedelta object)\n",
    "        method   = imputation method\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        dff = imputed regularly spaced [pandas dataframe]\n",
    "        \"\"\"\n",
    "        # 0. preprocess: if dataframe alreay has time as index: reset and add as column\n",
    "        if df.index.name == \"ds\":\n",
    "            # df_lim[\"ds\"] = df_lim.index\n",
    "            df.reset_index(level=0, inplace=True)\n",
    "        \n",
    "        # 1. include in dataset times where you want to impute (and set to NaN values)\n",
    "        impute_times = self.fget_regular_times(df, timestep)\n",
    "        dft = self.finterleaf(df, impute_times)\n",
    "        \n",
    "        # 2. impute with chosen method\n",
    "        dfi = self.fimpute(dft, method=method)\n",
    "        \n",
    "        # 3. remove original data not at correct timestep\n",
    "        dff = dfi[dfi.index.isin(impute_times) == True]\n",
    "        \n",
    "        if dff.index.freq == None:\n",
    "            dff.index.freq = to_offset(timestep)\n",
    "        \n",
    "        return dff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class cStationary(cPreProcessing):\n",
    "    \"\"\"\n",
    "    methods for checking whether time series is stationary\n",
    "    methods for transforming the time series into a stationary time series\n",
    "    methods for obtaining (p,q,d) ARIMA parameters\n",
    "    \n",
    "    https://towardsdatascience.com/detecting-stationarity-in-time-series-data-d29e0a21e638\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fadf_verbose(self, adftest):\n",
    "        \"\"\"\n",
    "        CHECK STATIONARITY.\n",
    "        Print explanation of output of Augmented Dickey-Fuller test.\n",
    "        \n",
    "        The Augmented Dickey-Fuller test is a type of statistical test called a unit root test. \n",
    "        The intuition behind a unit root test is that it determines how strongly a time series is defined by a trend.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        adftest = adfuller(data.y, regression=\"ct\")\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        print(\"\"\"\n",
    "        Augmented Dickey-Fuller:\n",
    "        Null hypothesis: the time series can be represented by a unit root, thus not stationary (has some time-dependent structure)\n",
    "        \"\"\")\n",
    "   \n",
    "        output = pd.Series(adftest[0:4], index=['Test Statistic','pvalue','#Lags Used','Number of Observations Used'])\n",
    "        for key,value in adftest[4].items():\n",
    "            output['Critical Value ({})'.format(key)] = value\n",
    "\n",
    "        print(output)\n",
    " \n",
    "        if output.pvalue <= 0.05:\n",
    "            print(\"\\nReject the null hypothesis (H0), the data does not have a unit root and IS STATIONARY.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"\\nFail to reject the null hypothesis (H0), the data has a unit root and is NON-STATIONARY.\")\n",
    "            return False\n",
    "\n",
    "    def fkpss_verbose(self, kpsstest):\n",
    "        \"\"\"\n",
    "        CHECK STATIONARITY.\n",
    "        Print explanation of output of Kwiatkowski-Phillips-Schmidt-Shin test.\n",
    "        \n",
    "        Another test for checking the stationarity of a time series (reversed null hypothesis to ADF).\n",
    "        In KPSS test, to turn ON the stationarity testing around a trend, you need to explicitly pass the regression='ct'.\n",
    "        A major difference between KPSS and ADF tests:\n",
    "        the capability of the KPSS test to check for stationarity in the ‘presence of a deterministic trend’.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        kpsstest = kpss(data.y, regression=\"ct\")\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        print(\"\"\"    \n",
    "        Kwiatkowski-Phillips-Schmidt-Shin:\n",
    "        Null hypothesis: the process is trend stationary\n",
    "        \"\"\")\n",
    "    \n",
    "        output = pd.Series(kpsstest[0:3], index=['Test Statistic','pvalue','Lags Used'])\n",
    "        for key,value in kpsstest[3].items():\n",
    "            output['Critical Value ({})'.format(key)] = value\n",
    "        print (output)\n",
    "\n",
    "        if output.pvalue <= 0.05:\n",
    "            print(\"\\nReject the null hypothesis (H0), the data has a unit root and is NON-STATIONARY.\")\n",
    "            return False\n",
    "        else:\n",
    "            print(\"\\nFail to reject the null hypothesis (H0),the data does not have a unit root and IS STATIONARY. \")    \n",
    "            return True\n",
    "        \n",
    "    def fstationary_verbose(self, stat_adf, stat_kpss):\n",
    "        \"\"\"\n",
    "        CHECK STATIONARITY.\n",
    "        Compare results of adf and kpss tests and advise how to make stationary.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \"\"\"\n",
    "        if (stat_adf is False) and (stat_kpss is False):\n",
    "            print(\"\\nBoth tests conclude that the series is not stationary -> series is not stationary\")\n",
    "            return False\n",
    "        elif (stat_adf is True) and (stat_kpss is True):\n",
    "            print(\"\\nBoth tests conclude that the series is stationary -> series is stationary\")\n",
    "            return True\n",
    "        elif (stat_adf is False) and (stat_kpss is True):\n",
    "            print(\"\\nKPSS = stationary and ADF = not stationary -> trend stationary, use power tranform to make stationary\")\n",
    "            return False\n",
    "        elif (stat_adf is True) and (stat_kpss is False):\n",
    "            print(\"\\nKPSS = not stationary and ADF = stationary -> difference stationary, use differencing transform to make stationary\")\n",
    "            return False\n",
    "\n",
    "        \n",
    "    def fcheck_stationary(self, y, verbose=True):\n",
    "        \"\"\"\n",
    "        CHECK STATIONARITY.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y = time series variable, data.y\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        stationary status [bool]\n",
    "        \"\"\"\n",
    "        #df.dropna()\n",
    "        \n",
    "        adftest  = adfuller(y, regression=\"ct\")\n",
    "        kpsstest = kpss(y, regression=\"ct\")\n",
    "        \n",
    "        if verbose:\n",
    "            stat_adf  = self.fadf_verbose(adftest)\n",
    "            stat_kpss = self.fkpss_verbose(kpsstest)\n",
    "            stat = self.fstationary_verbose(stat_adf, stat_kpss)\n",
    "\n",
    "        return stat\n",
    "    \n",
    "    def fdecompose(self, df, model=\"additive\"):\n",
    "        \"\"\"\n",
    "        CHECK STATIONARITY.\n",
    "        \n",
    "        Seasonal decomposition using moving averages\n",
    "        https://www.statsmodels.org/stable/generated/statsmodels.tsa.seasonal.seasonal_decompose.html\n",
    "        Time series must be regularly spaced (have constant frequency, dff.index.freq or dff.index.inferred_freq)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df = data frame with date index (to infer frequency)\n",
    "        \"\"\"\n",
    "        s = seasonal_decompose(df, model=model)\n",
    "        \n",
    "        trend = s.trend\n",
    "        plt.plot(trend)\n",
    "        plt.title(\"Trend\")\n",
    "        plt.show()\n",
    "\n",
    "        seasonal = s.seasonal\n",
    "        plt.plot(seasonal)\n",
    "        plt.title(\"Seasonal component\")\n",
    "        plt.show()\n",
    "\n",
    "        resid = s.resid\n",
    "        plt.plot(resid)\n",
    "        plt.title(\"Residuals\")\n",
    "        plt.show()\n",
    "        \n",
    "    def fcheck_density(self, y):\n",
    "        \"\"\"\n",
    "        CHECK STATIONARITY.\n",
    "        Plot histogram and density trend (check gaussianity)\n",
    "        \"\"\"\n",
    "        plt.figure(1)\n",
    "        plt.subplot(211)\n",
    "        plt.hist(y)\n",
    "        plt.title(\"Data Distribution\")\n",
    "        plt.subplot(212)\n",
    "        y.plot(kind='kde')\n",
    "        plt.show()\n",
    "    \n",
    "    def fcheck_lag(self, y):\n",
    "        \"\"\"\n",
    "        CHECK STATIONARITY.\n",
    "        Plot lag scatter, autocorrelation and partial autocorrelation functions\n",
    "        For differencing and establishing (p,q,d) values for ARIMA\n",
    "        \"\"\"\n",
    "        plt.figure()\n",
    "        pd.plotting.lag_plot(y)\n",
    "        plt.title(\"Lag-1 plot\")\n",
    "        plt.plot()\n",
    "\n",
    "        plt.figure()\n",
    "        pd.plotting.autocorrelation_plot(y)\n",
    "        plt.title(\"Autocorrelation\")\n",
    "        plt.plot()\n",
    "\n",
    "        # contains confidence interval: \n",
    "        # correlation values outside of this code are very likely a correlation and not a statistical fluke\n",
    "        plot_acf(y)\n",
    "        plot_pacf(y)\n",
    "    \n",
    "    def fdifferencing(self, df, interval=1):\n",
    "        \"\"\"\n",
    "        MAKE STATIONARY. (difference stationary)\n",
    "        \n",
    "        adf and kpss can give the d value required by ARIMA\n",
    "        \n",
    "        Make series stationary: In order to satisfy the assumption, it is necessary to make the series stationary. \n",
    "        This would include checking the stationarity of the series and performing required transformations\n",
    "        Determine d value: For making the series stationary, the number of times the difference operation was \n",
    "        performed will be taken as the d value.\n",
    "        \n",
    "        The auro_arima function works by conducting differencing tests \n",
    "        (i.e., Kwiatkowski–Phillips–Schmidt–Shin, Augmented Dickey-Fuller or Phillips–Perron) \n",
    "        to determine the order of differencing, d. Canova-Hansen test for seasonal stability.\n",
    "        \n",
    "        https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.diff.html\n",
    "        \"\"\"\n",
    "        # df.dropna()\n",
    "        \n",
    "        return df.diff(periods=interval)\n",
    "\n",
    "    def fboxcox(self, df):\n",
    "        \"\"\"\n",
    "        MAKE STATIONARY. (trend stationary)\n",
    "        \n",
    "        [https://mode.com/example-gallery/forecasting_prophet_python_cookbook/]\n",
    "        Often in forecasting, you’ll explicitly choose a specific type of power transform to apply to the data \n",
    "        to remove noise before feeding the data into a forecasting model (e.g. a log transform or square root \n",
    "        transform, amongst others). However, it can sometimes be difficult to determine which type of power \n",
    "        transform is appropriate for your data. This is where the Box-Cox Transform comes in. Box-Cox Transforms \n",
    "        are data transformations that evaluate a set of lambda coefficients (λ) and selects the value that \n",
    "        achieves the best approximation of normality.\n",
    "        \n",
    "        Prophet natively models the increase in mean of the data over time, \n",
    "        but we should take additional steps to normalize as much variance as possible \n",
    "        to achieve the most accurate forecasting results.\n",
    "        We can do this by applying a power transform to our data.\n",
    "        \n",
    "        [https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html]\n",
    "        \n",
    "        inverse transform (after fitting and forecasting):\n",
    "        forecast[['yhat','yhat_upper','yhat_lower']] = forecast[['yhat','yhat_upper','yhat_lower']].apply(lambda x: inv_boxcox(x, lam))\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df = dataframe with y component to be transformed\n",
    "             constraints: data must be positive (and non-zero)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        df  = original dataframe with additional tranformed y column\n",
    "        lam = lambda value of power transform determined by boxcox, needed for inversing the transformation\n",
    "              constraints: lambda must not be negative\n",
    "        \"\"\"\n",
    "        df['y_trans'], lam = boxcox(df['y'])\n",
    "        \n",
    "        return df, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "class cErrorMetrics():\n",
    "    \"\"\"\n",
    "    methods for evaluating error\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fmse_manual(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Mean square error.\n",
    "        Manual implementation.\n",
    "        \"\"\"\n",
    "        return np.sum( (np.array(y_true) - np.array(y_pred))**2 ) / len(y_true)\n",
    "    \n",
    "    def fmse(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Mean square error.\n",
    "        (from sklearn.metrics import mean_squared_error)\n",
    "        \"\"\"\n",
    "        return mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    def frms(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Root mean square error.\n",
    "        (from sklearn.metrics import mean_squared_error)\n",
    "        \"\"\"\n",
    "        return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    \n",
    "    def fmape(self, y_true, y_pred): \n",
    "        \"\"\"\n",
    "        mean_absolute_percentage_error.\n",
    "        (can cause division-by-zero errors)\n",
    "        \"\"\"\n",
    "        y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "        return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    def fcc(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        correlation coefficient\n",
    "        as defined in Gruet et al (2018)\n",
    "        (cov is a 2x2 matrix?)\n",
    "        \"\"\"\n",
    "        n = np.cov(y_true, y_pred)\n",
    "        d = np.sqrt(np.var(y_true)*np.var(y_pred))\n",
    "        return n/d\n",
    "\n",
    "    def fcc_pearsonr(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        pearson correlation coefficient\n",
    "        \"\"\"\n",
    "        return pearsonr(y_true, y_pred)\n",
    "    \n",
    "    def faic(self):\n",
    "        \"\"\"\n",
    "        Akaike information criterion.\n",
    "        An estimator of out-of-sample prediction error and thereby relative quality of statistical models for a given set of data.\n",
    "        Method of model selection (AutoArima)\n",
    "        \"\"\"\n",
    "        #statsmodels.tools.eval_measures.aic\n",
    "        pass\n",
    "    \n",
    "    def fbic(self):\n",
    "        \"\"\"\n",
    "        Bayes information criterion, Schwarz criterion.\n",
    "        Criterion for model selection among a finite set of models; the model with the lowest BIC is preferred.\n",
    "        \"\"\"\n",
    "        #statsmodels.tools.eval_measures.bic\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted 01_stats_utils.ipynb.\n",
      "Converted 02_plot_utils.ipynb.\n",
      "Converted 03_read_data.ipynb.\n",
      "Converted 04_model.ipynb.\n",
      "Converted 05_solar_flux_time_series_forecasting.ipynb.\n",
      "Converted 06_ensemble_utils.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
